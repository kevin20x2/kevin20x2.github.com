<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[分类:  cnn | ZVINK's garden]]></title>
  <link href="http://kevin20x2.github.io/blog/categories/cnn/atom.xml" rel="self"/>
  <link href="http://kevin20x2.github.io/"/>
  <updated>2018-01-20T15:44:43+08:00</updated>
  <id>http://kevin20x2.github.io/</id>
  <author>
    <name><![CDATA[ZVINK]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Scalpel]]></title>
    <link href="http://kevin20x2.github.io/blog/2018/01/10/scalpel/"/>
    <updated>2018-01-10T12:10:05+08:00</updated>
    <id>http://kevin20x2.github.io/blog/2018/01/10/scalpel</id>
    <content type="html"><![CDATA[<!-- mathjax config similar to math.stackexchange -->


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<!--more-->


<h3>SCALPEL对于硬件平台的分类</h3>

<ul>
<li>低并行性硬件平台 (微控制器， ARM)</li>
<li>高并行性硬件平台 (CPU，DRAM 系统)</li>
<li>中等并行性硬件平台  (GPU)</li>
</ul>


<p>对于低并行性硬件，采用</p>

<h2><strong>SIMD-aware weight pruning</strong></h2>

<p><img src="https://github.com/kevin20x2/kevin20x2.github.com/blob/master/images/SIMD_aware_pruning.png?raw=true" alt="" /></p>

<p>要点: <br/>
1. 对权重进行分组 ，根据SIMD的宽度来分组 (Cortex-M4 分成2组)
2. 裁剪权重，根据每一组的均方根来裁剪， 当均方根小于某一个阈值就裁剪掉。(给的例子上数据存储方式采用 CSR 方式) <br/>
3. <strong>训练方式</strong>(这个跟我们以前做的有很大不同) : 每裁剪完一层就开始重训练，直到DNN 不能保持原有的准确度。<br/>
4. <strong>流程</strong>： SIMD-aware 权重裁剪一层一层的工作，最开始会计算每一层的的运行时间。 然后从运行时间最高的一层开始裁剪，每一次裁剪过后，将该层的运行时间更新。裁剪过后进行重训练，如果DNN没有丢失准确度，进行下一次迭代，即，重新选择最慢的一层进行裁剪。<br/>
5. 关于dropout率的调整 : 将dropout 看作对于神经元的采样，对于层 $i$ ,$C_i$ 是连接的数量， $C_io$ 是原始网络， $C_ir$是remaining 网络的连接。</p>

<p>$$D_r = D_o\sqrt{ {\frac{C_ir}{C_io}} } $$<br/>
$D_o$ 是原始的dropout 率，$D_r$ 是裁剪过后的调整的dropout率。</p>

<ol>
<li>采用修改的CSR 格式，包括三个1D 数组 ，$A&#8217;,IA&#8217;,JA&#8217;$ .<br/>
$A&#8217;$ 中以原始顺序存放了所有非零权重组。<br/>
$IA&#8217;$ 记录了$A&#8217;$ 中每一行第一组非零 权重的编号 。
$JA&#8217;$ 记录了 $A&#8217;$中每一组的列数编号。</li>
</ol>


<h3>总结：</h3>

<p>SIMD-aware 权重裁剪 在低并行平台上 同时减少了模型大小和 运行时间，每个权重组使用同一个列编号戏剧性的减少了$JA&#8217;$的存储量。 对于计算，使用一个SIMD指令同时load 多个连续的input 能减少计算指令个数。 <br/>
实验表明，
基本裁剪到60%左右，速度就比原始的要快。(具体实验对比不细说)</p>

<h2>Node Pruning</h2>

<p>Node Pruning 主要针对 GPU 平台 ， <br/>
将全连接层的<strong>一个神经元</strong> 和 卷积层的 一个 <strong>特征图</strong>  看作一个node 。<br/>
删除掉nodes只会减少每一层的大小，但是不会影响网络中的稀疏程度。因此删掉一个node不会影响 计算速度 (说白了就是结构化裁剪)</p>

<p><img src="https://github.com/kevin20x2/kevin20x2.github.com/blob/master/images/Mask_layer.png?raw=true" alt="" /></p>

<h4>流程:</h4>

<ul>
<li>给除了input 和 output layer 之外的层添加 mask layer 。<br/>
图中给一个全连接层加上了mask layer 。A 中的每一个node 在输出结果之前需要通过mask layer $A&#8217;$ ，每一个node 有两个参数 : $\alpha$ 和 $\beta$ , $\alpha$ 是 布尔变量，取值为0或1 ， $\beta$ 是0到1之间的浮点值。令数组
$Y$ 和 $Y&#8217;$为 origin layer和mask layer 的输出，有:</li>
</ul>


<p>$$ y&#8217;_i = \alpha_i * y_i$$</p>

<p>当 $\alpha_i$被设为0的时候 ，相关node 就相当于是被裁剪掉了。  <br/>
* 重训练mask layer ，$\alpha$ 和 $\beta$ 都被初始化为1.</p>

<p><img src="https://github.com/kevin20x2/kevin20x2.github.com/blob/master/images/update_alpha.png?raw=true" alt="update_aplha" /><br/>
T 是所有mask layer 都维护的一个阈值 ，$\epsilon$ 是一个比较小的值，为了防止$\alpha$ 会不停从0到1切换，$\beta_i$ 通过反向传播更新，并且被裁剪到 [0,1] 范围 。</p>

<ul>
<li>使用L1 正则化来控制裁剪的node 的数量。 对于每一个mask layer ，每一个参数 $\beta_i$ 的惩罚值可以这样计算 。</li>
</ul>


<p>$$R_{i,L1} = \lambda |\beta_i| = \lambda \beta_i$$</p>

<p>$\lambda$ 是weight decay (正则化长度) ，它让 $\beta_i$ 接近0.如果与之相关的节点不重要，$\beta_i$ 就会变得比阈值 T 要小，然后就会被暂时 remove。 因为有后来发现 被 remove 的节点比较重要的这种状况，所以mask layer 的参数仍然参与重训练。 T 在node pruning 中是固定值，因此提高 $\lambda$ 会导致更多的节点被裁剪。</p>

<ul>
<li>在训练完 mask layer 之后 ， L1 正则化的weight decay会增加，这样在下一次迭代的时候，就会裁剪掉更多的node 。 如此重复，直到DNN 的准确度不能恢复。</li>
<li>node pruning 的最后一步是删除掉 masked nodes ，不相关的数据都删除掉，最后能得到一个裁剪之后的DNN .</li>
</ul>


<h2>Combined Pruning</h2>

<p>对于像是CPU 架构的平台，可以结合SIMD-aware weight pruning 和 Node Pruning 。 这里将DNN的batch size 设置为1. <br/>
   在CPU 平台上， Scalpel 对于全连接层采取 SIMD-aware weight pruning ，对于 卷积层采取 node pruning 。</p>

<h2>结论</h2>

<p>On the microcontroller, CPU and GPU, Scalpel achieves
mean speedups of 3.54x, 2.61x, and 1.25x while reducing the model
sizes by 88%, 82%, and 53%.</p>

<p>感觉这个思路非常的赞啊！</p>

<div id="container"></div>


<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>

<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>


<script>
var gitment = new Gitment({
  id: 'location.href', // 可选。默认为 location.href
  owner: 'kevin20x2',
  repo: 'blog_comment',
  oauth: {
    client_id: '0d172133d26f520041e6',
    client_secret: '14eef65983bd9d0e16313feaffcf1684fc3713a9',
  },
})
gitment.render('container')
</script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SC-DCNN]]></title>
    <link href="http://kevin20x2.github.io/blog/2018/01/07/sc-dcnn/"/>
    <updated>2018-01-07T15:41:20+08:00</updated>
    <id>http://kevin20x2.github.io/blog/2018/01/07/sc-dcnn</id>
    <content type="html"><![CDATA[<!-- mathjax config similar to math.stackexchange -->


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$']],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    messageStyle: "none",
    "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<p>SC-DCNN : ASPLOS 2017</p>

<h2>Stochastic Computing (SC) :</h2>

<!--more-->


<p>感觉是一个很有趣的 <strong>数据表示方法</strong><br/>
其数字是基于概率表示的比如 ：</p>

<pre><code>1011011101
</code></pre>

<p>令该字节流表示[0,1] 中的浮点值
这个字节流中有10个bit ，然后其中有7个值为1 ，其中值为1的概率为0.7  即:</p>

<h3>P(X = 1) = 7/10 = 0.7</h3>

<p>如果要让其表示 [-1,1] 之间的浮点值则因为</p>

<h3>P(x = 1) = (0.4 + 1 )/2 =7/10</h3>

<p>因此该字节流表示 0.4。</p>

<p>使用SC的一个很大的好处是乘法的计算变得非常容易，如果采用[-1,1]的表示方式，可以用异或门简单的得到出乘法结果。</p>

<p><img src="https://github.com/kevin20x2/kevin20x2.github.com/blob/master/images/XOR.png?raw=true" alt="xor" /></p>

<p>但是代价是加法变得更加复杂,文中提到了4种计算SC中加法的方法</p>

<ol>
<li>用或门来计算</li>
<li>用Mux 单元计算</li>
<li>用Approximate parallel counter(APC) 来进行计算</li>
<li>还有用两行表示法来计算的方法</li>
</ol>


<p>文中从理论和实验将1和4 排除，最终的实现方法只用了Mux 和APC 两种。</p>

<p>用Mux 计算加法稍微有点不好理解，因为数据表示方法就是基于概率论的，该方法也只是概率上相加。</p>

<p><img src="https://github.com/kevin20x2/kevin20x2.github.com/blob/master/images/MUX.png?raw=true" alt="MUX" /></p>

<p>如图所示: Ai ,Bi 是输入流，Ci是输出流，Si是一个随机值 <br/>
对于Si的要求是MUX的每个输入通道的概率加起来的值等于1.即这里 Si每次的值要么是0，要么是1，为0的时候Ci = Ai ，为1 的时候 Ci = Bi。<br/>
 这里当然取 P(Si = 0) = P( Si = 1) = 0.5 。假设一个数字的长度为n，那么在n个周期之后，输出的序列值C就是 &frac12; 的(A + B) 的和。这个结论可以由概率论得到 .
 <code>
    c = 2P(C = 1 ) -1 = 2(1/2P(A = 1)+1/2P(B = 1)) - 1 = 1/2 (a + b)
</code>
这个时候就会发现这种方式跟传统的计算方式有很大的不同，基于CPU 和GPU 的传统方式虽然也是描述概率问题，但是其本身的计算过程跟概率是没有关系的，也就是对于相同的输入数据，运行多少次结果应该都是不变的。</p>

<p><img src="https://github.com/kevin20x2/kevin20x2.github.com/blob/master/images/APC.png?raw=true" alt="APC" /></p>

<p>APC 正如其名 ，是并行的的计数器， 可以直接统计n个输入中，有多少个0 .假设其输入通道的个数为n，那么每次得到bit 为 $log_2 n$  .</p>

<p>至于激活层，文中选择了tanh ，因为这是最容易实现的一种方法。</p>

<p><img src="https://github.com/kevin20x2/kevin20x2.github.com/blob/master/images/tanh.png?raw=true" alt="tanh" /></p>

<p>如图所示，有限状态机(FSM)一位位的读取数据，当当前位为1的时候转移到下一个状态，否则返回到前一个状态，然后当前状态在前K/2的时候输出1，否则输出 0.<br/>
这样就将 输出的就是 $tanh(k/2*x)$ (具体原理没有深究)</p>

<p>文中还提出了一种计算最大池化的方法，这里不细讲 。</p>

<p>最后是实验结果，这种用硬件+ 概率的方式 在准确度上应该是比不过CPU和GPU这种精确计算的方式的。<br/>
其重点在于能耗上面，只能说这种方式能耗跟其他的方式基本上都不在一个量级上，论文中只实现了简单的Lenet-5 . <br/>
<strong>提到了一个比较重要的说法是，虽然小网络精确度没有传统方法高，但是随着数据变多，网络变复杂，这种基于概率的方法准确度反而会上升</strong>。</p>
]]></content>
  </entry>
  
</feed>
